---
title: PCA with R
author: Laci Cartmell
date: 11/02/2022
output:
    html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## Understanding PCA

Interactions between variables are often hidden especially w/complex and/or large datasets. PCAs are often used to visualize large datasets because they reduce complexity while retaining important information. 

How do 10 variables interact with each other variable? We could make 45 pairwise scatterplots, but looking at them would be time consuming - or we could use PCA to find explore those interactions for us. 

#
#### So, what exactly is PCA?

Principal Components Analysis (PCA) is a statistical test that reduces dimensionality while preserving variance by linearly transforming data into new coordinate system with fewer dimensions than the initial data.

PCA takes in data as a dataframe. Then, it is centered and scaled which is the basis of the new coordinate plane.     
 
The transformation fits data into a coordinate system with the most variance is in the first coordinate (component), the 2nd most variance in the second coordinate, and so on. 

Each principal component is now a summary of the variables from original data. Principal components are linear combinations of the original predictors. 

Because of the orthogonal transformation the data undergoes, numerical data works the best. You could code categorical as continuous but better to use categorical variables later on for grouping (plus pretty colors)

# 
#### PCA does multiple things at once: 

- Exploratory analysis 
    - Variable reduction - we find the total variance in the original dataset explained by each principal component
    -  do our categorical variables group?
# 

- Model selection
  -  variable reduction
  -  reduce dimensionality - esp if vars are correlated
  -  PCR - Principle Component Regression

The goal of the PCA is to explain the most variability with fewer variables than we started with. If we can capture most of the variation in two dimensions, then we can project all the observations onto a scatterplot known as a biplot. However, if a majority of the variation isn't explained by the first two components, adding the 3rd pc may be necessary. But how does that change our PCA and the visualization of it?

## PCA in R

```

install.packages("ggfortify")
install.packages("devtools")
install.packages("remotes") 

# Iris dataset - 3 species, 50 samples each
str(iris)
summary(iris)

# Create factor var w/levels for species category
iris$Species <- factor(iris$Species,
                       levels = c("versicolor", "virginica",
                                  "setosa"))
summary(Iris_pca)   # Remove species

# remove categorical variable
Iris_pca <- subset(iris, select = -c(Species))

# Check correlation (usually done earlier)
round(cor(iris[,1:4]), 2)

# PCA
Iris_pca <- prcomp(Iris_pca,
                   center = TRUE,   # mean of 0
                   scale. = TRUE)   # STD of 1
summary(Iris_pca)
str(Iris_pca)
  

#Plotting a PCA - checks for linearity and normality
library(ggfortify)

#plots PC1,PC2 automatically
iris_pca_plot <- autoplot(Iris_pca,
                          data = iris,
                        #  colour = 'Species'
                          )
iris_pca_plot


# screeplot - how many components to keep
plot_iris_pca <- plot(Iris_pca, type="lines")


# biplot - how components were combined in 2-d
biplot_iris_pca <- biplot(Iris_pca)

```

## PCA in 3-dimensions

The screeplot is also used to decide if our data fits a PCA. Sometimes, a PCA won't explain enough variance from only 2 components to be incredibly helpful. 



https://planspacedotorg.wordpress.com/2013/02/03/pca-3d-visualization-and-clustering-in-r/
February 3, 2013 by aaronschumacher
```
#####

Iris_pca <- prcomp(iris[,1:4],
                   center = TRUE,   # mean of 0
                   scale. = TRUE,   # STD of 1 
                   cor=TRUE,
                   scores=TRUE)

library(rgl)


plot3d(Iris_pca$scores[,1:3])
text3d(pc$scores[,1:3],texts=rownames(iris))
text3d(pc$loadings[,1:3], texts=rownames(pc$loadings), col="red")
coords <- NULL
for (i in 1:nrow(pc$loadings)) {
  coords <- rbind(coords, rbind(c(0,0,0),pc$loadings[i,1:3]))
}

# not great tools for 3d in R, but we can look at the 3 pcs with most variance

set.seed(42)
cl <- kmeans(iris[,1:4],3)
iris$cluster <- as.factor(cl$cluster)

plot3d(pc$scores[,1:3], col=iris$cluster, main="k-means clusters")
plot3d(pc$scores[,1:3], col=iris$Species, main="actual species")

with(iris, table(cluster, Species))


```






